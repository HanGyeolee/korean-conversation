[README](https://github.com/HanGyeolee/korean-conversation)

# Update
* [22.08.31](#220831) - 주요소 태그 변경 및 설명
* [22.01.01](#220101) - 주요소 추출 및 표현 방식 변경
* [21.12.29](#211229) - 주요소 추출 방식 변경
* [21.10.26](#211026) - 주요소 태그 모델 출력 13개로 확대
* [21.10.21](#211021) - 빈 토큰 추가
* [21.10.17](#211017) - 문장 구성 데이터
* [21.10.15](#211015) - 토크나이저 최적화
* [21.10.13](#211013) - 주요소 태그 클래스 구현
* [21.10.12](#211012) - 주요소 태그 형태
* [21.10.10](#211010) - 토크나이저 클래스 구현

## 22.08.31
주요소 속성 변경 및 해당 주요소에 들어가는 값들 설명

'어떻게'에 해당하는 H 제거

|주요소|조건|
|:-:|:--|
|V|서술어 = [용언, (무엇이다, 어찌하다, 어떠하다), 동사, 형용사, 체언+서술격 조사(이다, 한다)]|
|S|주어 = [문장의 주체(누가, 무엇이), 체언+주격 조사(이, 가, 에서[단체], 께서[높임])]|
|DO|직접목적어 = [타동사의 상대 또는 목표나 목적(누구를, 무엇을), 체언+목적격 조사 (을,를), 체언+보조사(도, 만)]|
|IO|간접목적어 = [부사어 중 타동사의 행동을 받는 체언+부사격 조사(에게, 한테, 께[높임])]|
|CO|보어 = [(서술절 안은 문장, 무엇이),체언+주격 조사(이, 가). (되다, 아니다)]|
|Adv|부사어 = [(부사절 안은 문장, 용언, 관형사, 부사, 문장 전체 수식), 부사만 (체언+부사격 조사 제외)]|
|Adj|관형어 = [(관형사절 안은 문장, 관형사), 체언+관형격 부사 (의), 동사+관형사형 어미, 형용사+관형사형 어미(ㄴ/는/ㄹ)]|
|T|시간 = [시간과 관련된 모든 체언 및 조사]|
|Wy|이유 = [이유와 관련된 부사절 안은 문장 (때문에,아서,으니까,~하자)]|
|WS|어디에서 = [부사어 중 위치를 나타내는 체언+부사격 조사(에서)]|
|WE|어디로 = [부사어 중 위치를 나타내는 체언+부사격 조사(으로, 로, 에)]|
|Wi|누구와 = [부사어 중 (와, 과, 함께)]|
|Ind|독립어 = [대등적 이어진 문장, 감탄사, 체언+호격 조사, 접속 부사]|

## 22.01.01
ex) 꽃이 이쁘게 피어있는 것을 가만히 바라보았다.

분해 시

`['꽃/NNG', '이/JKS', '이쁘/VA', '게/EC', '피/VV', '어/EC', '있/VX', '는/ETM', '것/NNB', '을/JKO', '가만히/MAG', '바라보/VV', '았/EP', '다/EF', './SF']`

역순

`['./SF', '다/EF', '았/EP', '바라보/VV', '가만히/MAG', '을/JKO', '것/NNB', '는/ETM', '있/VX', '어/EC', '피/VV', '게/EC', '이쁘/VA', '이/JKS', '꽃/NNG']`

객체 표현
```JSON
{
    "log": "2022.01.01.6.01:56:14.894934",
    "raw": "꽃이 이쁘게 피어있는 것을 가만히 바라보았다.",
    "morpheme": ["꽃/NNG", "이/JKS", "이쁘/VA", "게/EC", "피/VV", "어/EC", "있/VX", "는/ETM", "것/NNB", "을/JKO", "가만히/MAG", "바라보/VV", "았/EP", "다/EF", "./SF"],
    "reversed": ["./SF", "다/EF", "았/EP", "바라보/VV", "가만히/MAG", "을/JKO", "것/NNB", "는/ETM", "있/VX", "어/EC", "피/VV", "게/EC", "이쁘/VA", "이/JKS", "꽃/NNG"],
    "sentencetype": "statement",
    "predicate": {
        "tense": "past",
        "adj": null,
        "type": "VV",
        "word": "바라보",
        "adv": [
            {
                "type": "MAG",
                "word": "가만히",
                "adv": null
            }
        ],
        "complement": null,
        "with": null,
        "indirectobject": null,
        "directobject": {
            "type": "NNB",
            "word": "것",
            "adv": [
                {
                    "tense": "present",
                    "adj": {
                        "type": "VX",
                        "word": "있",
                        "adv": null
                    },
                    "type": "VV",
                    "word": "피",
                    "adv": [
                        {
                            "type": "VA",
                            "word": "이쁘",
                            "adv": null
                        }
                    ],
                    "complement": null,
                    "with": null,
                    "indirectobject": null,
                    "directobject": null,
                    "whereend": null,
                    "wherestart": null,
                    "how": null,
                    "why": "?",
                    "time": "?",
                    "subject": {
                        "type": "NNG",
                        "word": "꽃",
                        "adv": null
                    }
                }
            ]
        },
        "whereend": null,
        "wherestart": null,
        "how": null,
        "why": "?",
        "time": "?",
        "subject": {
            "type": null,
            "word": "?",
            "adv": null
        }
    }
}
```

전체폼
```JSON
{
    "log":          "인식한 시간",
    "raw":          "입력된 문장",
    "morpheme":     [ "형태소 분리된 문장" ],
    "reversed":     [ "분리된 문장을 역순으로" ],
    "sentencetype": "statement(평서문) | question(의문문) | exclamation(감탄문) | command(명령문)",
    "predicate":    {}
}
```

서술어 폼
```JSON
{
    "tense":        "시제",
    "adj":          "보조 동사",
    "type":         "동사 | 형용사",
    "word":         "형태소 분할된 단어",
    "adv":          [ "형용사", "부사", "등" ],
    "complement":   "주격 보어 | 목적격 보어",
    "with":         "함께",
    "indirectobject": "간접 목적어",
    "directobject": "직접 목적어",
    "whereend":     "행동이 끝나는 위치",
    "wherestart":   "행동이 시작되는 위치",
    "how":          "방법",
    "why":          "이유",
    "time":         "시간",
    "subject":      "주어"
}
```

나머지
```JSON
{
    "type":         "품사",
    "word":         "단어",
    "adv":          [ "형용사", "부사", "등" ]
}
```

위와 같이 형식을 만들어보는 것은 어떤 가 해서 생각해 보았습니다.
모든 조사를 생략하지 않는 다는 조건 하에, 동사를 꾸미는 형용사나 목적어로 완성된 문장이 오는 겹문장에 대한 예외를 해결할 수 있을 거라 생각했습니다.

주요소를 추출하는 방식을 다양하게 변경하는 이유는 데이터셋을 만드는 게 너무 힘든 나머지 알고리즘으로 해결하고자 여 방법을 써보고 있는 중입니다.

## 21.12.29
주요소 추출하는 방식을 인공지능에서 단순 쿼리로 변경하였습니다.
조사는 다음과 같은 데이터를 쿼리해서 주요소를 가져옵니다.

|주요소|쿼리조건|
|:-:|:--|
|S|가/JKS,은/JX,는/JX|
|T|지금,시간,어제,오늘,잠깐,아침,시/NNBC,내일,분/NNBC,옛날/NNG,아까/MAG,이후/NNG,주로/MAG|
|Wy|데/NNB,아서/EC,으니싸/EC|
|H| ~~ |
|WS|에서/JKB|
|WE|으로/JKB,에/JKB,로/JKB|
|DO|을/JKO,를/JKO|
|IO|에게,께/JKS,한테/JKB|
|Wi|랑/JKB,이랑/JKB,와/JKB,과/JKB,함께/MAG|
|CO|으로/JKB,로/JKB|

방식을 변경한 결과는 다음과 같습니다.

![image](https://user-images.githubusercontent.com/46367614/147605096-cc1af5bc-ab98-420c-a14d-e9234fe18d50.png)

쿼리가 겹치는 `으로/JKB` `로/JKB` 의 경우는 동사에 필요한 주요소에 따라 결정됩니다.
주로 `WE`와 `CO`가 동시에 필요한 문장 형식이 거의 없는 것으로 판단해서 아직은 괜찮다고 생각합니다.
만약 동사 중에 `WE`와 `CO`가 모두 필요한 문장 형식이 있다면 예외처리에 대한 부분을 고민해봐야 합니다.

하지만 아직 조사가 없는 경우 예외처리를 어떻게 해결해야 할지 고민입니다.

## 21.10.26
주요소 태그 인공지능 모델의 출력을 12개에서 13개로 늘렸습니다.    
배칭을 하면서 입력의 크기가 동일해야 한다는 단점이 있었고, 입력의 크기를 동일하게 하기 위해서는 사용하지 않는 데이터(여기서는 토큰 0)를 포함해야 합니다. 그리하여 결과 부분에서도 1~12의 주요소 태그들을 뿐만 아니라 0에 해당하는 사용하지 않는(여기서는 이미 문장을 넘어서는 'EOF' 다음 글자) 데이터를 포함시켰습니다.     
모델을 학습할 때 배치사이즈는 16으로 하였고, GPU 메모리 누수를 대비하여 autocast 를 사용하였습니다.    
그라디언트 축적(Gradient Accumulation)을 사용하니 전체적인 학습 속도는 느려졌지만, 0.1 이하의 Loss 까지 학습이 진행되는 것을 확인하였습니다.


## 21.10.21
토크나이저에 사용자가 추가로 단어를 입력할 수 있도록 하였습니다.    
이전 634405개의 토큰에서 비어있는 토큰 20954개를 추가하여, 총 655360개의 토큰을 이용하여 학습을 진행합니다.    
사용자가 언제든지 새로운 단어를 추가해서 입력할 수 있습니다.

현재 서술어에 따른 주요소 데이터 csv 파일을 정리 중입니다. 국립국어원 표준국어대사전을 이용하여 데이터 셋을 만드는 중인데 대략 한달 반 정도 소요될 것으로 예상합니다.
추가적으로 주요소 태거 학습을 위한 데이터 셋은 정리하는 데에만 한 세월이 걸릴 것으로 예상중입니다.

## 21.10.17
[Principler](https://github.com/HanGyeolee/korean-conversation/blob/main/principler.py#L11)를 통해서 세분화된 단어들 중 서술어만을 가져올 수 있습니다.
가져온 서술어를 기반으로 주요소 데이터를 뽑아올 수 있는 데, 반드시 알아야하는 데이터를 '?'로 표시하고 부가적인 데이터를 '&#42;'로 표현하였습니다.

|V|S|T|Wy|WS|WE|DO|IO|H|Wi|CO|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|가다|?|?|?|&#42;|?|&#42;|&#42;|&#42;|&#42;|&#42;|
|가려워하다|?|?|?|&#42;|&#42;|&#42;|&#42;|&#42;|&#42;|&#42;|

각각 해당하는 단어에 필요한 추가적인 내용 (~에/에게, ~을, ~으로) 등은 [국립국어원 표준국어대사전](https://stdict.korean.go.kr/main/main.do)에 작성된 문형 정보를 토대로 데이터를 작성하였습니다. 뜻이 여러개를 가지는 동사 (예: 가리다.) 의 경우, 가리다0, 가리다1 과 같이 구분을 하였으나 해당 단어를 어떻게 자동으로 선택할지는 아직 고민해봐야할 문제입니다.

다음은 현재까지의 작동 예시입니다.     
4번째 출력에서 'WS' 가 'WE' 로 출력이 되어야 정상작동하는 것입니다.     
아직 주요소 태거의 정확도가 높지 않은 탓입니다.    
![Test](./example.png)

## 21.10.15
+ 토크나이저를 전반적으로 수정하였습니다.    
    모든 숫자가 ${number}/SN 하나로 토크나이징 되는 것 처럼 :    
    * 모든 동사 => '${verb}/VV'    
    * 모든 형용사 => '${adjective}/VA'      
    * 모든 종결기호 => '${mark}/SF'      
    * 모든 종결어미 => '${end}/EF'

    주요소를 판별하는 데, 동사의 세부적인 정보는 필요가 없기 때문에 간략하게 표현하였습니다.
    대신 '어제', '오늘'과 같은 시간을 나타내는 명사들은 그대로 'NNG' 태그가 붙으며, 주요소에 제일 중요한 관계언 또한 그대로 각각의 태그가 붙습니다.
    또한, 한자로 시작하는 모든 단어를 토크나이저에서 제외시켰습니다. 이를 통해 816293개에 해당하는 토큰들을 634405개로 단축시킬 수 있었습니다.

+ 주요소에 사용되는 태그에 '보어'를 추가했습니다.    
    주격보어와 목적격보어를 나타낼 태그가 없었기 때문에 추가했습니다.

    |V|S|T|Wy|
    |:---:|:---:|:---:|:---:|
    |서술어|주어|시간|이유|

    |WS|WE|DO|IO|H|Wi|CO|EOF|
    |:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
    |어디에서|어디로|직접목적어|간접목적어|어떻게|누구와|보어|./?/!|

+ [21.10.13](#211013)의 예시 수정

## 21.10.13
[ELEMENTagger](https://github.com/HanGyeolee/korean-conversation/blob/main/mecab/ko/elemen.py)는 문장의 주요소를 추가로 태깅해줍니다.    
``` python
from mecab.ko.tokenizer import Tokenizer
from mecab.ko.elemen import ELEMENTagger

tokenizer = Tokenizer(dicpath=r'vocab.txt') 
tokenizer.tokenizing(string, allattrs=False) #토큰 생성

elementagger = ELEMENTagger(ptpath="~~.pt", vocab_size=tokenizer.getMax() + 1)
elementagger.getElement(tokenizer.tokens) #주요소 추출
```

[공식 튜토리얼 사이트](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)를 참고하여 학습기를 완성시켰습니다.    
아직 데이터셋이 부족하여, Loss 가 2 이상 발생하는 중입니다. (말뭉치를 요청하여 데이터 구축을 준비하는 중입니다.)    

|항목|데이터셋 예시|
|:---:|:---|
|text| 	&#91;'어제/MAG', '부모/NNG', '님/XSN', '께/JKB', '편지/NNG', '를/JKO', '쓰/VV', '었/EP', '어/EF', './SF'&#93; |
|element|	&#91;'T', 'IO', 'IO', 'IO', 'DO', 'DO', 'V', 'V', 'V', 'EOF'&#93;|
|start|	&#91;1, 2, 2, 2, 5, 5, 7, 7, 7, 10&#93;|
|end|	&#91;1, 4, 4, 4, 6, 6, 9, 9, 9, 10&#93;|
|length|10|

## 21.10.12
POSTagger 뿐아니라 문장의 주요소를 태깅하여 쉽게 정보를 뽑아오도록 하려고 ELEMENTagger를 만드는 중입니다.    
LSTM을 이용하여 양방향 태깅을 위한 시퀀스 레이블링을 따라 구현해보았습니다.    
참고한 사이트는 [02. 양방향 RNN을 이용한 품사 태깅](https://wikidocs.net/66747)과 [사용자 정의 DATASET, DATALOADER, TRANSFORMS 작성하기](https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html)입니다.    

주요소에 사용되는 태그들은 다음과 같습니다.
* 중요도 높음

|V|S|T|Wy|
|:---:|:---:|:---:|:---:|
|서술어|주어|시간|이유|
    
* 중요도 낮음  
 
|WS|WE|DO|IO|H|Wi|EOF|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|어디에서|어디로|직접목적어|간접목적어|어떻게|누구와|./?/!|

문장 속 형태소 최대 개수는 512개로 제한하였습니다.    
데이터 셋을 만들다가 부족하다싶으면 더 늘려볼 생각입니다.

## 21.10.10
형태소를 뽑아내는 데에 [KoNLPy](https://konlpy.org/ko/latest/) Mecab 클래스를 사용하였습니다.    
mecab 클래스를 약간 수정하여 "+" 된 어절을 전부 분리하도록 하였습니다.    
[mecab/mecab_tokenizer.py](https://github.com/HanGyeolee/korean-conversation/blob/main/mecab/ko/tokenizer.py)를 통해 분리된 형태소들을 토크나이징할 수 있습니다.    
숫자는 ${number}/SN 로 토크나이징 될 것입니다.

mecab-ko-dic의 모든 어절을 vocab.txt에 저장해두었습니다.    
vocab.txt에 들어간 전체 어휘의 개수는 816293개 입니다.

<b>Ex)</b>    
> 경기 성남시 판교신도시에서 이달 분양하는 중대형 아파트의 3.3m²당 분양가가 2006년보다 200만 원 정도 싼 1500만 원 후반대로 결정될 것으로 보인다.

``` json
[
  218296, 724373, 317499, 396668, 812437, 254248, 191715, 350259, 249027, 291460, 
  816276, 4587, 372880, 816256, 10582, 191764, 816287, 816288, 816287, -1, 
  -1, 816164, 291460, 816137, 191481, 816287, 207053, 191664, 816287, 418926, 
  207370, 365195, 767213, 816284, 816287, 418926, 207370, 413800, 191548, 217693, 
  816266, 816285, 206811, 191746, 771756, 816286, 765791
]
```
-1은 vocab.txt에 포함되지 않은 단어들입니다. 위의 예시에서는 m와 ²가 각각 -1 값을 반환하는 데, 해당 데이터를 무작정 넣어봐야하는 지 고민입니다.    
~~[test.py의 result](https://github.com/HanGyeolee/korean-conversation/blob/main/test.py#L38) 값에 따라 반환되는 형태를 변경할 수 있으니 여러가지로 활용해볼 수 있겠습니다.~~
